{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "t5tr30nV_9Tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58bcbba-75b3-475e-dd40-5f2580cd2605"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install editdistance --quiet"
      ],
      "metadata": {
        "id": "_oYLI4C8CX4Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import json"
      ],
      "metadata": {
        "id": "6HOKQNN1RTje"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have datakey .json document stored openai api key make this line active to run. Otherwise, make comment here and activate the next line by pasting your api key\n",
        "\n",
        "OPENAI_API_KEY = ''\n",
        "with open('datakey.json', 'r') as file_to_read:\n",
        "    json_data = json.load(file_to_read)\n",
        "    OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]\n",
        "\n",
        "openai.api_key =  OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "MD3aDmfRACRX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make active this to run if you just have api key instead \n",
        "\n",
        "#openai.api_key =  \"Your api key\""
      ],
      "metadata": {
        "id": "zFlIbZo8v1mL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "\n",
        "import re\n",
        "\n",
        "import random\n",
        "import time\n",
        "import threading\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "!pip install zemberek-python\n",
        "from zemberek import TurkishMorphology\n",
        "from collections import OrderedDict\n",
        "morphology = TurkishMorphology.create_with_defaults()\n",
        "\n",
        "import editdistance\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n"
      ],
      "metadata": {
        "id": "yyjqaN1nhHKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c49f1a-1b41-4f13-c2e9-24702e109b6f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting zemberek-python\n",
            "  Downloading zemberek_python-0.2.3-py3-none-any.whl (95.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8 (from zemberek-python)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from zemberek-python) (1.22.4)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=740e7198d9f2e5bc269577d46399a612ec795d0acb04ac9e63d21cf4b46e7ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zemberek-python\n",
            "Successfully installed antlr4-python3-runtime-4.8 zemberek-python-0.2.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:zemberek.morphology.turkish_morphology:TurkishMorphology instance initialized in 15.38664984703064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-01 18:28:22,933 - zemberek.morphology.turkish_morphology - INFO\n",
            "Msg: TurkishMorphology instance initialized in 15.38664984703064\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing and Importing Wordnet\n",
        "\n",
        "!pip install git+https://github.com/starlangsoftware/TurkishWordNet-Py\n",
        "\n",
        "from WordNet.WordNet import WordNet\n",
        "\n",
        "a = WordNet()"
      ],
      "metadata": {
        "id": "p7Q9Fr28rM8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3afb76-9531-4e03-b501-c5630cf14157"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/starlangsoftware/TurkishWordNet-Py\n",
            "  Cloning https://github.com/starlangsoftware/TurkishWordNet-Py to /tmp/pip-req-build-o5hep4ua\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/starlangsoftware/TurkishWordNet-Py /tmp/pip-req-build-o5hep4ua\n",
            "  Resolved https://github.com/starlangsoftware/TurkishWordNet-Py to commit 3f7d82bba5918ac4efa826efaeb6921d730bb7ba\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting NlpToolkit-MorphologicalAnalysis (from NlpToolkit-WordNet==1.0.24)\n",
            "  Downloading NlpToolkit-MorphologicalAnalysis-1.0.47.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting NlpToolkit-Dictionary (from NlpToolkit-MorphologicalAnalysis->NlpToolkit-WordNet==1.0.24)\n",
            "  Downloading NlpToolkit-Dictionary-1.0.34.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting NlpToolkit-Corpus (from NlpToolkit-MorphologicalAnalysis->NlpToolkit-WordNet==1.0.24)\n",
            "  Downloading NlpToolkit-Corpus-1.0.25.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting NlpToolkit-DataStructure (from NlpToolkit-MorphologicalAnalysis->NlpToolkit-WordNet==1.0.24)\n",
            "  Downloading NlpToolkit-DataStructure-1.0.14.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting NlpToolkit-Math (from NlpToolkit-Dictionary->NlpToolkit-MorphologicalAnalysis->NlpToolkit-WordNet==1.0.24)\n",
            "  Downloading NlpToolkit-Math-1.0.18.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NlpToolkit-WordNet, NlpToolkit-MorphologicalAnalysis, NlpToolkit-Corpus, NlpToolkit-DataStructure, NlpToolkit-Dictionary, NlpToolkit-Math\n",
            "  Building wheel for NlpToolkit-WordNet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-WordNet: filename=NlpToolkit_WordNet-1.0.24-py3-none-any.whl size=11656509 sha256=be0df9622d0e8a53ed0000cce73f52a18021a4d26c4e0333be73b1c64d91bb9d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pnvokqpb/wheels/22/14/11/741e14fc5d118e96a2984fa8ab672fe7260c2e57e72981ab78\n",
            "  Building wheel for NlpToolkit-MorphologicalAnalysis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-MorphologicalAnalysis: filename=NlpToolkit_MorphologicalAnalysis-1.0.47-py3-none-any.whl size=63212 sha256=d2d6eba4dc75b95a43b3b5f2879d0d3b1e8f70103166e5bcfc32659fa5690e78\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/17/da/eb2062d3efd3f84ec1fa752fcfc886b8238c4063527e878cce\n",
            "  Building wheel for NlpToolkit-Corpus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-Corpus: filename=NlpToolkit_Corpus-1.0.25-py3-none-any.whl size=27668 sha256=9effc783ce58545662432a00c6d1da2fd7744e18d6b6a026fbfd2dd6ceef8f28\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/e0/72/616785dea01e638b274405fa7d314bd8e2b64f7fbfa9dd19bc\n",
            "  Building wheel for NlpToolkit-DataStructure (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-DataStructure: filename=NlpToolkit_DataStructure-1.0.14-py3-none-any.whl size=23762 sha256=84bc290772bc78a3f94ea15e78062f18acc820e757b3635a4b8fd1bce0351ca1\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c2/ff/c54f7a5400ef906937fd1ee01296415555c31b8ebfc47dbb22\n",
            "  Building wheel for NlpToolkit-Dictionary (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-Dictionary: filename=NlpToolkit_Dictionary-1.0.34-py3-none-any.whl size=1357917 sha256=ab0371a96f9c0c2b7716b278e2c39ba14ef9a394b0bc8c7ccfaf6188d1a1dfeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/8f/01/6af1297e548b2b5b595bfe73f57667c19bdfe2c8affe8001e7\n",
            "  Building wheel for NlpToolkit-Math (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NlpToolkit-Math: filename=NlpToolkit_Math-1.0.18-py3-none-any.whl size=30307 sha256=4382c92aa8a48ef61587979625df887bb77d26b7e69f89a5b43a91982718c6ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/24/80/f8aefdfa0b8b7c3cef7b3a2b24fc2ed86d781c88eb48125d28\n",
            "Successfully built NlpToolkit-WordNet NlpToolkit-MorphologicalAnalysis NlpToolkit-Corpus NlpToolkit-DataStructure NlpToolkit-Dictionary NlpToolkit-Math\n",
            "Installing collected packages: NlpToolkit-Math, NlpToolkit-DataStructure, NlpToolkit-Dictionary, NlpToolkit-Corpus, NlpToolkit-MorphologicalAnalysis, NlpToolkit-WordNet\n",
            "Successfully installed NlpToolkit-Corpus-1.0.25 NlpToolkit-DataStructure-1.0.14 NlpToolkit-Dictionary-1.0.34 NlpToolkit-Math-1.0.18 NlpToolkit-MorphologicalAnalysis-1.0.47 NlpToolkit-WordNet-1.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the raw URL of the CSV file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/selimfirat/bilkent-turkish-writings-dataset/master/data/texts.csv'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "corpus = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "spPG-dP0CNHo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Model Selection # Note if the current model is overloaded with requests, please change the other one\n",
        "\n",
        "model_type = \"gpt-3.5-turbo\" #1\n",
        "\n",
        "#model_type = \"gpt-3.5-turbo-0301\" #2\n"
      ],
      "metadata": {
        "id": "NTeEd67w74M-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Head\n",
        "\n",
        "def Head():\n",
        "  head = \"\"\"\n",
        "\n",
        "  _        _______          _________     _______           _______ _________\n",
        "  ( \\      (  ____ \\|\\     /|\\__   __/    (  ____ \\|\\     /|(  ___  )\\__   __/\n",
        "  | (      | (    \\/( \\   / )   ) (       | (    \\/| )   ( || (   ) |   ) (   \n",
        "  | |      | (__     \\ (_) /    | | _____ | |      | (___) || (___) |   | |   \n",
        "  | |      |  __)     ) _ (     | |(_____)| |      |  ___  ||  ___  |   | |   \n",
        "  | |      | (       / ( ) \\    | |       | |      | (   ) || (   ) |   | |   \n",
        "  | (____/\\| (____/\\( /   \\ )___) (___    | (____/\\| )   ( || )   ( |   | |   \n",
        "  (_______/(_______/|/     \\|\\_______/    (_______/|/     \\||/     \\|   )_(   \n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  check = 0\n",
        "  while True:\n",
        "      time.sleep(0.01)\n",
        "      print(head)\n",
        "      time.sleep(0.5)\n",
        "      clear_output()\n",
        "      check += 1\n",
        "      if check == 10:\n",
        "        print(head)\n",
        "        break\n"
      ],
      "metadata": {
        "id": "KIWQAHN_RYXe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions and start info. # ALDIK\n",
        "def StartCommand():\n",
        "    instructions = \"\"\"\n",
        "                                                                LEXI-CHAT Kelime Oyununa Hoşgeldiniz.\n",
        "\n",
        "    LEXI-CHAT, basit doğal dil işleme teknikleriyle ve doğal dil işleme modeli yapay zeka Chatgpt-3.5 Turbo ile desteklenmiş bir kelime tahmin etme oyunudur.\n",
        "\n",
        "    Oyuna başlamadan önce lütfen kuralları ve oyun sürecinde, oyundan keyif almanızı sağlayacak adımları içeren aşağıdaki maddeleri okuyunuz.\n",
        "\n",
        "    1- Bu oyun bir televizyon programı olan \"Kelime Oyunu\"ndan ilham alarak hazırlanmış ve size sunucunun varlığını hissettirecek şekilde tasarlanmıştır.\n",
        "    2- Oyunda, size verilen 5 dakikayı kullanarak bilmeniz gereken 4 harflilerden 10 harfli kelimelere kadar her birinden iki tane olacak şekilde toplam 14 soru bulunmaktadır.\n",
        "    3- Her bir harf değeri 100 puan olmakla beraber oyundan alabileceğiniz maksimum puan  9800 puandır.\n",
        "    4- Oyunda, herhangi bir kelime için \"h\" yazarak harf talep edebilirsiniz, her açılan harf için o kelimenin size kazandıracağı puandan 100 puan eksilir.\n",
        "    5- Oyun süresince size verilen mesaj kutucuğundan, soru öncesi ya da sırasında kelime ile ilgili ek bilgi talep edebilir ve tahminlerinizi yazabilirsiniz.\n",
        "    6- Yazdığınız tahminlere ya da içinde bulunduğunuz puan durumuna göre sunucu size ek ipuçları verebilir.\n",
        "    7- Tahminlerinizden birinin doğru olduğunu düşünüyorsanız yanıtınız kabul edilmesi için önce \"bb\" yazıp akan süreyi durdurmalı ve açılan yeni kutucukta yanıtınızı belirtmelisiniz.\n",
        "    8- 14 soru da bittikten sonra ya da oyun süresinin tamamını sorulardan önce bitirdiğinizde oyun sonlanacaktır ve toplam puanınız gösterilecektir.\n",
        "\n",
        "    NOT: Oyundan yüksek verim almak için lütfen oyun sunucusunu aldatmaya yönelik iletişim kurmaktan çekininiz. Evet, siz sunucuyu kandırabilir, hata vermesine neden olabilir ve kırabilirsiniz ancak\n",
        "    lütfen bir gün onun da sizi kırabileceğini hatırlayın.\n",
        "\n",
        "                                                                            KEYİFLİ OYUNLAR\n",
        "    \"\"\"\n",
        "\n",
        "    startcommand = \"\"\"\n",
        "\n",
        "                                                  Oyunu başlatmak için bölmeye \"Ve kayıt\" yazabilirsiniz.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(instructions)\n",
        "    time.sleep(4)\n",
        "    print(\"\\r\", startcommand, end=\"\")\n",
        "    \n",
        "    while True:\n",
        "      global start_com\n",
        "      start_com = input(\"->: \")\n",
        "      if start_com.strip() in [\"Ve kayıt\",\"ve kayıt\"]:\n",
        "        break\n",
        "      else:\n",
        "        print(\"Henüz oyunda değilsiniz, başlamak için \\\"Ve kayıt\\\" yazınız.\")\n",
        "\n",
        "    name = input(\"Lütfen isminizi girin: \")\n",
        "    address = input(f\"Size nasıl hitap etmemi istersiniz? {name} (hanım/bey): \")\n",
        "    global username\n",
        "    username = name + \" \" + address.lower()\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "vY5d0lhhQBAb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic input tokenizer\n",
        "\n",
        "def input_tokenizer(text):\n",
        "  return nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "LvJCswKKGmBW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic punctiation cleaner\n",
        "\n",
        "def clean_string(text):\n",
        "  pattern = re.compile(r'[^\\w\\s]')\n",
        "  return re.sub(pattern, \"\", text)\n"
      ],
      "metadata": {
        "id": "sx4MjVhpUyzC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT | Function for the origin of the words -GPT included\n",
        "\n",
        "def origin_of_the_words(word_list):\n",
        "  \"\"\"This function takes a list of words and returns their origin as a list with the same indexes\"\"\"\n",
        "\n",
        "   # Prompt given to Chatgpt for the origin of the words. This will be computed before the game start.\n",
        "\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Sana verilen kelimenin etimolojik kökenini yalnızca yalnızca iki kelime kullanarak söyleyen bir asistansın, bilgiyi etimoloji.com'dan al.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Bundan sonra vereceğim kelimeler için yanıtları yalnızca örnek formatta ver, başka ekleme yapma (Örnek: \\\"Arapça kökenli\\\").\"},\n",
        "  ]\n",
        "\n",
        "  not_cleaned_origin_list = []\n",
        "  origin_list = []\n",
        "  for item in word_list:\n",
        "    text=item\n",
        "    message={\"role\":\"user\",\"content\":text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create( model=f\"{model_type}\", messages=messages, max_tokens=7 )\n",
        "    resp = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    no_punc = clean_string(resp)\n",
        "    cleaned = resp.lower().replace(item, \"\")\n",
        "    not_cleaned_origin_list.append(cleaned)\n",
        "    messages.append(response[\"choices\"][0][\"message\"])\n",
        "\n",
        "  for phrase in not_cleaned_origin_list:\n",
        "      match = re.search(r'(.*)\\s?kökenli', phrase)\n",
        "      if match:\n",
        "          origin_list.append(match.group(1).strip().capitalize()+ \" kökenli\")\n",
        "      else:\n",
        "          origin_list.append(\"None\")\n",
        "\n",
        "  return origin_list\n",
        "\n"
      ],
      "metadata": {
        "id": "LWeH7T1VO6Pn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT | Function for the structure info of the words | Compound or Individual -GPT included\n",
        "\n",
        "def Structure_of_Word(word_list):\n",
        "  \"\"\"This function takes a list of words and returns their structure info as a list with the same indexes\"\"\"\n",
        "\n",
        "   # Prompt given to Chatgpt for the structure info of the words. This will be computed before the game start.\n",
        "\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Verilen kelimenin birleşik kelime mi tek kelime mi olduğunu söyle. Yalnızca bilgiyi \\\"Birleşik kelime\\\" ya da \\\"Tek kelime\\\" olarak ver, başka bir şey yazma.\"},\n",
        "      {\"role\": \"user\", \"content\": \"sıradağ\"},\n",
        "  ]\n",
        "\n",
        "  not_cleaned_structure_list = []\n",
        "  \n",
        "  for item in word_list:\n",
        "    text=item\n",
        "    message={\"role\":\"user\",\"content\":text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create( model=f\"{model_type}\", messages=messages, max_tokens=10 )\n",
        "    resp = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    no_punc = clean_string(resp)\n",
        "    cleaned = resp.lower().replace(item, \"\")\n",
        "    not_cleaned_structure_list.append(cleaned)\n",
        "    messages.append(response[\"choices\"][0][\"message\"])\n",
        "\n",
        "  if len(not_cleaned_structure_list) > 14:\n",
        "    del not_cleaned_structure_list[0]\n",
        "  filtered_list = [re.findall(r'birleşik kelime|tek kelime', i) for i in not_cleaned_structure_list]\n",
        "\n",
        "  structure_list = [i[0] for i in filtered_list]\n",
        "\n",
        "  return structure_list\n",
        "\n"
      ],
      "metadata": {
        "id": "TSvfj6YYaQTw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This GPT Prologue function. Makes a short conversation before starting and asks user whether they are ready.\n",
        "\n",
        "def Gpt_Prologue():\n",
        "  print(f\"Merhaba {username}, Lexichat'e hoşgeldiniz!\")\n",
        "\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Sen Lexichat adlı bir Kelime Oyunu yarışması sunucususun Yarışma öncesi kısa bir sohbet gerçekleştirmek senin amacın. Önemli olan, diyaloğu yarışmacıyı rahatlatmak için yapman. Daha çok sohbet ağzı benimse ancak mutlaka \\\"siz\\\" diye hitap et. Birkaç karşılıklı diyalog gerçekletikten sonra yarışma için hazır olup olmadığını soracaksın.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Merhaba\"},\n",
        "  ]\n",
        "  response = openai.ChatCompletion.create( model=f\"{model_type}\", messages=messages, max_tokens=100 )\n",
        "  print(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "  print(\"Bu arada hazır olduğunuzda her zaman \\\"hazırım\\\" yazabilirsiniz.\")\n",
        "  tokenized_inp = []\n",
        "  while True:\n",
        "    text=input(\"--> \")\n",
        "    \n",
        "    if \"hazırım\" in input_tokenizer(text.lower()) or \"evet hazırım\" in input_tokenizer(text):\n",
        "      print(\"Öyleyse başlayabiliriz... Şöyle derin bir nefes alın...\")\n",
        "      break\n",
        "    message={\"role\":\"user\",\"content\":text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create( model=f\"{model_type}\", messages=messages, max_tokens=100 )\n",
        "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "    messages.append(response[\"choices\"][0][\"message\"])\n",
        "\n",
        "  time.sleep(2)\n",
        "  print(\"Nefesi aldınız mı..?\")\n",
        "  time.sleep(0.5)\n",
        "  print(\"Artık başlamaya hazırsınız sanırım?\")\n",
        "  text = input(\"--> \")\n",
        "  if text in [\"hazırım\", \"hazır\", \"başla\", \"basla\", \"evet\"]:\n",
        "    print(\"İlk soruyla başlıyorum öyleyse...\\n\")"
      ],
      "metadata": {
        "id": "9kfHJmImSGAV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for word function (V, N, ADV, etc.)\n",
        "def function_of_word(word_list):\n",
        "  function_list = [\"None\"]*14\n",
        "  for i in range(len(word_list)):\n",
        "    if get_POS(word_list[i]) != \"None\":\n",
        "      function_list[i] = get_POS(word_list[i]) \n",
        "  return function_list\n"
      ],
      "metadata": {
        "id": "8okEz14C02mT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Letter request function\n",
        "\n",
        "revealed_letters = []\n",
        "def LetterRequest(word):\n",
        "  import random\n",
        "\n",
        "  word = word.strip()\n",
        "  word_sliced = [None] * len(word)\n",
        "\n",
        "  for i in range(0, len(word)):\n",
        "    word_sliced[i] = word[i]\n",
        "    if len(revealed_letters) != len(word):\n",
        "      revealed_letters.append(\"_  \")\n",
        "\n",
        "  random_num = random.randint(0, len(word)-1)\n",
        "  if revealed_letters[random_num] == \"_  \":\n",
        "    revealed_letters[random_num] = word_sliced[random_num]\n",
        "  elif \"_  \" not in revealed_letters:\n",
        "    print(\"Efendim, tüm harfleri açtınız, artık bir sonraki soruya...\\nHarf alamazsınız artık.\")\n",
        "    \n",
        "  else:\n",
        "    LetterRequest(word)\n",
        "  return revealed_letters"
      ],
      "metadata": {
        "id": "F-yp3lSOOhaA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Rihcz3sy2OkA"
      },
      "outputs": [],
      "source": [
        "# Round_Time function\n",
        "\n",
        "def Round_Time(round):\n",
        "  global active_input_bb\n",
        "  active_input_bb = \"\"\n",
        "  countdown_time = 45\n",
        "  check_ = 0\n",
        "  \n",
        "  def check_input():\n",
        "      print(\"\")\n",
        "      chance_list = [2, 3, 4, 7, 9, 5, 25, 65]\n",
        "      list_active_input_bb = []\n",
        "      while True:\n",
        "        if check_ == 1:\n",
        "          break\n",
        "        else:\n",
        "          global active_input_bb\n",
        "          active_input_bb = input(\"-BB> \")\n",
        "          tkn_input_bb = input_tokenizer(active_input_bb.lower())\n",
        "          if active_input_bb.lower().strip() == word:\n",
        "            if chosen_phrase == \"Bana mı soruyorsunuz, cevap mı veriyorsunuz?..\":\n",
        "              phrases = [\"Cevap veriyorlar..!\", \"Sanırım cevap veriyorsunuz ve doğru olanı yapıyorsunuz\"]\n",
        "              print(\"\\n\",phrases[random.randint(0, (len(phrases)-1))]) \n",
        "            elif chosen_phrase == f\"{word} sığıyor mu oraya?\":\n",
        "              print(\"\\nEyvah, eyvah! Efendim sığıyor mu ki?...\")\n",
        "              time.sleep(3)\n",
        "              print(\"Tabii ki sığıyor! Yalnızca biraz heyecanlandırmak istedim ancak buna kanmadılar kendileri...\") \n",
        "            \n",
        "            round_score = (LetterRequest(word).count(\"_  \") + 1) * 100\n",
        "            congrats_phrases = [f\"Tebrik ederim efendim {word} doğru cevap ve size {round_score} puan kazandırıyor...\", f\"Bir {round_score} puan geliyor...\", f\"Bu soruyla birlikte toplam puanınıza bir {round_score} puan daha eklediniz...\", f\"{round_score} puan cepte\", f\"Tertemiz bir {round_score} puan...\", f\"{round_score} puanı kasamıza ekliyoruz...\", f\"{word}, size {round_score} kazandırıyor\"]\n",
        "            print(\"\\n\",congrats_phrases[random.randint(0,(len(congrats_phrases) - 1))])\n",
        "            global total_score\n",
        "            total_score += round_score\n",
        "            break\n",
        "          elif (active_input_bb.lower().strip() == \"h\") or (\"harf\" in tkn_input_bb and (\"alabilir\" in tkn_input_bb or \"alayım\" in tkn_input_bb)):\n",
        "            print(\"\\nEfendim, harf alamazsınız artık, süreyi durdurdunuz.\")\n",
        "\n",
        "          elif any(item in [\"kök\", \"köken\", \"kökenli\", \"kökeni\"] for item in tkn_input_bb):\n",
        "            if Origin_list[round] != \"None\":             \n",
        "              origin_phrases = [f\"Sanırım {Origin_list[round]} olmalı\", f\"Hmm... Bu, sanıyorum {Origin_list[round]}\", f\"{Origin_list[round]}\", f\"{Origin_list[round]} olma ihtimali yüksek\"]\n",
        "              print(\"\\n\",origin_phrases[random.randint(0, (len(origin_phrases) - 1))]) \n",
        "            else:\n",
        "              print(\"\\nMaalesef kökeninden emin değilim...\")\n",
        "\n",
        "          elif any(item in [\"yardım\", \"yardımcı\", \"ipucu\", \"destek\", \"bulamadım\", \"bilemedim\"] for item in tkn_input_bb):\n",
        "\n",
        "            if len(chance_list) == 0:\n",
        "              negative_phrases = [\"Maalesef, daha fazla yardımcı olamam...\", \"Efendim, daha ne diyeyim ki... Bulursunuz siz bunu...\", \"Başka bir şey gelmiyor ki aklıma...\", \"Maalesef..! Odaklanın, bulursunuz...\\nSoruyu bir daha okuyun isterseniz...\"]\n",
        "              print(\"\\n\",negative_phrases[random.randint(0, (len(negative_phrases)))]) \n",
        "            else:\n",
        "              chance_num = random.choice(chance_list)\n",
        "              \n",
        "              if chance_num % 3 == 0:\n",
        "                print(\"\\nÖrnek cümle verebilirim belki...\")\n",
        "                time.sleep(1)\n",
        "                print(\"\\n\",Example_sentences[round])\n",
        "                chance_list.remove(3)\n",
        "                chance_list.remove(9)\n",
        "\n",
        "              elif chance_num % 5 == 0:\n",
        "                if Compound_list[round] != \"None\":\n",
        "                  print(f\"\\nEfendim buna dikkat... {Compound_list[round]}\")\n",
        "                elif Additional_defs[round] != None:\n",
        "                  print(f\"\\nŞöyle de tanımlanabilir...\\n{Additional_defs[round]}\")\n",
        "                else:\n",
        "                  print(f\"\\nBiraz daha düşünün efendim, tekrar okuyun soruyu...\\n{definition_list[round]}\")\n",
        "\n",
        "                chance_list.remove(5)\n",
        "                chance_list.remove(25)\n",
        "                chance_list.remove(65)\n",
        "\n",
        "              elif chance_num % 2 == 0:\n",
        "                print(\"\\nHmm...\")\n",
        "                time.sleep(1)\n",
        "                try:\n",
        "                  print(\"\\n\",getSynonym(round))\n",
        "                except:\n",
        "                  print(\"\\n\",get_example_sentence(word))\n",
        "                chance_list.remove(2)\n",
        "                chance_list.remove(4)\n",
        "                time.sleep(1)\n",
        "                print(\"\\nDüşünün biraz daha...\")\n",
        "\n",
        "              elif chance_num % 7 == 0:\n",
        "                print(\"\\nHmm... Belki bu yardımcı olabilir...\")\n",
        "                time.sleep(1)\n",
        "                print(\"\\n\",Example_sentences[round])\n",
        "                time.sleep(2)\n",
        "                print(\"Ayrıca...\")\n",
        "                try:\n",
        "                  getSynonym(round)  \n",
        "                except:\n",
        "                  get_example_sentence(word)\n",
        "                chance_list.remove(7)\n",
        "                print(\"\\nOdaklanırsanız bulursunuz bence... Tekrar okuyun...\")\n",
        "\n",
        "          else:\n",
        "              string_familiarity = [similar_word_hint(word, 3)]\n",
        "              list_synonym = [synonym_hint(word)]\n",
        "              #return string_familiarity\n",
        "              #return list_synonym\n",
        "\n",
        "              if active_input_bb.lower().strip() not in list_active_input_bb:\n",
        "                string_familiarity = similar_word_hint(word, 3)\n",
        "                list_synonym = synonym_hint(word)\n",
        "                  \n",
        "              if active_input_bb.lower().strip() in string_familiarity and active_input_bb.lower().strip() in list_synonym:\n",
        "                print(\"\\nHadi bir daha, çok çok yaklaştınız...\")\n",
        "\n",
        "              elif active_input_bb.lower().strip() in string_familiarity or any(item in string_familiarity for item in tkn_input_bb):\n",
        "                print(\"\\nÇok yaklaştınız, hadi birkaç harf değiştirin.\")\n",
        "\n",
        "              elif active_input_bb.lower() == \"\":\n",
        "                phrases_list_empty = [\"Bir harf alabilirsiniz belki...\", \"...\", \"Zamana dikkat!\", \"Zaman akıyor efendim, bir harf mi istesek..?\", \"...\"]\n",
        "                print(\"\\n\",phrases_list_empty[random.randint(0, (len(phrases_list_else)-1))])              \n",
        "              \n",
        "              elif active_input_bb.lower().strip() in list_synonym or any(item in list_synonym for item in tkn_input_bb):\n",
        "                synonym_phrases = [\"Bu değil efendim, öteki... Eş anlamlısı lazım bize...\", \"Hadi bir daha deneyin... Bu sefer eş anlamlısını söyleyin...\", f\"{active_input_bb} değil de... Bir benzeri...\\nNeydi o?\\nBize eş anlamlısı lazım...\", \"Yaklaştınız... Aynı anlama gelen başka kelime daha var...\\nNeydi o..?\"]\n",
        "                print(\"\\n\",synonym_phrases[random.randint(0, (len(synonym_phrases) - 1))])\n",
        "\n",
        "              elif active_input_bb.lower().strip() in list_active_input_bb or any(item in list_active_input_bb for item in tkn_input_bb):\n",
        "                print(\"\\nBunu zaten söylediniz, tekrar düşünün...\")         \n",
        "\n",
        "              else: \n",
        "                if editdistance.distance(active_input_bb.lower().strip(), word) <= 2:\n",
        "                  if round >= 6:\n",
        "                    distance_phrases = [\"Çok yaklaştınız, acaba bir iki harf mi değiştirsek..?\", f\"{active_input_bb.lower()} doğru muydu yoksa birkaç harf mi farklıydı?\"] \n",
        "                    print(distance_phrases[random.randint(0, len(distance_phrases) - 1)])   \n",
        "                else:\n",
        "                  phrases_list_else = [\"Süre akıyor, tekrar deneyin...\", \"Biraz daha düşünün isterseniz...\", \"Ah, ah! Keşke bir harf daha alsaydınız...\", \"Zamana dikkat! Zamana dikkat..!\", \"Değil, değil... Zaman akıyor efendim...\", \"Hayır, hayır... Dikkatlice tekrar okuyun isterseniz soruyu...\"]\n",
        "                  print(\"\\n\",phrases_list_else[random.randint(0, (len(phrases_list_else)-1))])\n",
        "            \n",
        "      list_active_input_bb.append(active_input_bb)\n",
        "      chance_list = [2, 3, 4, 7, 9, 5, 25, 65]\n",
        "      revealed_letters = []\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "  input_thread = threading.Thread(target=check_input)\n",
        "  input_thread.start()\n",
        "\n",
        "  # Start the countdown\n",
        "  for i in range(countdown_time, 0, -1):\n",
        "    print(\"\\r\", i, end=\"\")  # Use '\\r' to overwrite the previous line\n",
        "    time.sleep(1)  # Wait for 1 second\n",
        "    if word.lower().strip() in active_input_bb.lower().strip().split():\n",
        "      break\n",
        "    if i == 1:\n",
        "      print(\"\\n\")\n",
        "      print(\"\\nÜzgünüm süreniz bitti, bakalım neymiş yanıt...\\n\")\n",
        "      time.sleep(1.5)\n",
        "      print(word_list[round])\n",
        "      round_score = (LetterRequest(word).count(\"_  \") + 1) * 100\n",
        "      global total_score\n",
        "      total_score -= round_score\n",
        "      negative_score_phrases = [f\"Bu soru size {round_score} puan kaybettirdi... Ama olsun toparlayabiliriz hâlâ...\", f\"Ah, ah... {total_score} puana geriledik...\\Moral bozmak yok devam edelim...\"]\n",
        "      print(negative_score_phrases[random.randint(0, len(negative_score_phrases) - 1)])\n",
        "      check_ = 1\n",
        "      break\n",
        "  input_thread.join()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT | Function for the example sentences of the words - GPT included\n",
        " \n",
        "def example_sentence(word_list):\n",
        "  \"\"\"This function takes a list of words and returns their 3 usage in a sentence as a list with the same indexes\"\"\"\n",
        "\n",
        "   # Prompt given to Chatgpt for creating example sentences of the words. This will be computed before the game start.\n",
        "\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Sana verilen kelime ile basit örnek bir cümle yaz verilen kelime yerine _______ koy. 3 kelimeden oluşsun.\"},\n",
        "      {\"role\": \"user\", \"content\": \"İlk kelime geliyor\"},\n",
        "  ]\n",
        "  \n",
        "  sentence_list = []\n",
        "\n",
        "  for item in word_list:\n",
        "    text=item\n",
        "    message={\"role\":\"user\",\"content\":text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create( model=f\"{model_type}\", messages=messages, max_tokens=200 )\n",
        "    \n",
        "    resp = (response[\"choices\"][0][\"message\"][\"content\"])\n",
        "    no_punc = clean_string(resp)\n",
        "    gg = resp.lower().replace(item, \"-------\")\n",
        "    sentence_list.append(gg)\n",
        "    messages.append(response[\"choices\"][0][\"message\"])\n",
        "\n",
        "  return sentence_list\n",
        "\n"
      ],
      "metadata": {
        "id": "lUS2CvsiPDDy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre_info function, information just before the next question --> Works, can be developed root info can be taken when demanded\n",
        "\n",
        "def Pre_info(question_number, word, total_score, round):\n",
        "  \"\"\" This function takes three inputs (question number, word, and total_score at that round), and returns phrases regarding these. This code requires random and openai libraries to run properly\"\"\"\n",
        "  import random\n",
        "  num = question_number\n",
        "  letter_count = (((question_number-1) // 2) + 4)\n",
        "  question_point = letter_count* 100\n",
        "  \n",
        "  # Prepared phrases for info\n",
        "  list_of_info_1 = [f\"{letter_count} harfli sorular ile devam ediyoruz efendim...\", f\"{question_number}. sorudayız...\", f\"{question_number}. soruya geldik...\"]\n",
        "  list_of_info_2 = [f\"Şu an {total_score} puandasınız, bu soru ile {total_score + question_point} puana ulaşabilirsiniz...\", f\"Sıradaki soru ile {question_point} puan sizi bekliyor efendim...\", f\"Sıradaki soru {question_point} puan değerinde...\"]\n",
        "  mumbling = [\"Hmm...\", \"Bakalım...\", \"Tamam...\", \"Şimdi bakalım...\", \"Şöyle ki...\", \"Hıh...\", \"Peki...\"]\n",
        "  list_of_info_first_q = [\"Tertemiz bir 400 puanla başlayabiliriz efendim...\", \"Güzel bir başlangıç yaparız umarım bu soruyla, dur bakalım...\", \"Şöyle silkinelim ve ilk soruyla başlayalım...\"]\n",
        "  \n",
        "  # Info for function of word\n",
        "  f_type = Function_list[round] \n",
        "  function_phrases = [f\"Bu kelime bir {f_type}...\", f\"Sıradaki kelime bir {f_type}...\", f\"Bu bir {f_type}...\"]\n",
        "\n",
        "  # Info for structure of word\n",
        "  s_type = Structure_list[round]\n",
        "  structure_phrases = [f\"Hmm, {s_type}den oluşan bir sözcük ile devam ediyoruz...\", f\"Sıradaki bir {s_type}, görelim neymiş...\", f\"Bu bir {s_type}... Bakalım neymiş...\", f\"Sonraki sözcük, {s_type}den oluşmakta...\"]\n",
        "\n",
        "  # Origin info        \n",
        "  output_root = Origin_list[round] \n",
        "  question_info = [f\"Sıradaki kelime {output_root}\", f\"Bu kelime sanırım {output_root}.. Dur bakalım çıkarabilecek misiniz...\", f\"Kelimenin kökenine baktığımda...\\nhmm sanırım {output_root} olmalı\", f\"Bu kelime, dilimize {output_root} bir kelimeden yerleşmiş.\"]\n",
        "\n",
        "  # Indexing for prepared phrases lists\n",
        "  index_info1 = random.randint(0, (len(list_of_info_1)-1))\n",
        "  index_info2 = random.randint(0, (len(list_of_info_2)-1))\n",
        "  index_mumbling = random.randint(0, (len(mumbling)-1))\n",
        "  index_question_info = random.randint(0, (len(question_info)-1))\n",
        "  index_info_first_q = random.randint(0, (len(list_of_info_first_q)-1))\n",
        "  index_f_type = random.randint(0, (len(function_phrases)-1))\n",
        "  index_s_type = random.randint(0, (len(structure_phrases)-1))\n",
        "  \n",
        "  # Info for the first question\n",
        "  if question_number == 1:\n",
        "    print(\"\\n\",list_of_info_first_q[index_info_first_q],\"\\n\")\n",
        "    time.sleep(0.5)\n",
        "    if f_type != \"None\":\n",
        "      print(f\"\\nBu kelime bir {f_type}...\")\n",
        "    time.sleep(0.5)\n",
        "    print(\"\\n\",mumbling[index_mumbling],\"\\n\")\n",
        "    time.sleep(1)\n",
        "    if s_type != \"None\":\n",
        "      print(f\"\\nBu bir {s_type}... Bakalım neymiş...\")\n",
        "  \n",
        "  # Info other than the first question\n",
        "  else:\n",
        "    print(\"\\n\",list_of_info_1[index_info1])\n",
        "    time.sleep(1)\n",
        "    print(\"\\n\",list_of_info_2[index_info2],\"\\n\")\n",
        "    time.sleep(1.2)\n",
        "    print(\"\\n\",mumbling[index_mumbling],\"\\n\")\n",
        "    time.sleep(1)\n",
        "    if f_type != \"None\":\n",
        "      print(\"\\n\",function_phrases[index_f_type])\n",
        "  \n",
        "    # Chance of give origin info\n",
        "    hint_chance = random.randint(1,5)\n",
        "    if hint_chance % 4 == 0:\n",
        "      if output_root != \"None\":    \n",
        "        print(\"\\n\",question_info[index_question_info])\n",
        "    time.sleep(1)\n",
        "    if hint_chance % 2 == 0:\n",
        "      if f_type != \"None\":   \n",
        "        print(\"\\n\",structure_phrases[index_s_type])\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "-e4bRTgsPY_r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Synonym | Returns synonym list\n",
        "\n",
        "def synonym_hint(word):\n",
        "  word_synset = a.getSynSetWithLiteral(word.lower(), 1)\n",
        "  try:\n",
        "    synonyms = str(word_synset.getSynonym())\n",
        "    synonym = re.sub(r'â', 'a', synonyms)\n",
        "    lists_synonym = synonym.split()\n",
        "    for x in lists_synonym:\n",
        "      if x == word:\n",
        "        lists_synonym.remove(x)\n",
        "    if len(lists_synonym) != 0:\n",
        "      return lists_synonym\n",
        "    else: \n",
        "      return \"None\"\n",
        "\n",
        "  except:\n",
        "    return \"None\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UnzLF9bAwD3N"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting synonyms | We will create the synonym_list \n",
        "\n",
        "def getSynonym(round):\n",
        "    synonym_list_copy = synonym_list.copy()  # Create a copy of the list\n",
        "  \n",
        "    if synonym_list_copy[round] == \"None\":\n",
        "        return \"None\"  # Return None if the synonym list is empty\n",
        "    num_synonyms = min(3, len(synonym_list_copy[round]))  # Determine the number of available synonyms\n",
        "    synonyms = random.sample(synonym_list_copy[round], num_synonyms)  # Select random synonyms\n",
        "    result = [f\"Belki bunlar size yardımcı olabilir: {', '.join(synonyms)}...\\nBu kelimeler eş anlamlı olarak kullanılır diyebilirim...\", f\"{', '.join(synonyms)}... Bunlara dikkat! Yardımcı olabilir...\", f\"{', '.join(synonyms)}... Bu kelimeler bir şeyler anımsatabilir...\", f\"{', '.join(synonyms)}... Bu kelimeleri düşünün...\"]\n",
        "    return result[random.randint(0, (len(result)-1))]  # Return the synonyms as a string separated by commas\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lSO2xwCP94te"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bilkent dataset and Function for edit distance\n",
        "\n",
        "#preprocessing of the bilkent dataset\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "cor = corpus.to_string(index=False)\n",
        "\n",
        "cor = cor.replace('\\n',\"\")\n",
        "#cor = re.sub(\"<[^\\<]>\", \"\", str(corpus))\n",
        "cor = re.sub(\"<[^\\<]Wd>\", \"\", str(corpus))\n",
        "\n",
        "cor = cor.lower()\n",
        "\n",
        "tokens = nltk.word_tokenize(cor)\n",
        "\n",
        "cleaned_tokens = []\n",
        "\n",
        "for word in tokens:\n",
        "  if not word.isnumeric() == True and not word.isalpha() != True and not len(word) <= 1:\n",
        "    cleaned_tokens.append(word)\n",
        "\n",
        "\n",
        "word_count = Counter(cleaned_tokens)\n",
        "sorted_keywords = list(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "#edit distance\n",
        "\n",
        "def similar_word_hint(word, n_suggestions):\n",
        "    \n",
        "    if word in cleaned_tokens:\n",
        "      return word\n",
        "    else:\n",
        "\n",
        "      distance = []\n",
        "      for token in cleaned_tokens:\n",
        "        d = editdistance.distance(word, token)\n",
        "        distance.append(d)\n",
        "\n",
        "      min_dist = min(distance)\n",
        "\n",
        "      candidates = {}\n",
        "      for x,y in sorted_keywords:\n",
        "        candidates[x] = editdistance.distance(word, x)\n",
        "        \n",
        "      suggestions = []\n",
        "      for candidate, dis in candidates.items():\n",
        "        if min_dist - 1 < dis <= min_dist:\n",
        "          if len(candidate) == len(word):\n",
        "            suggestions.append(candidate)\n",
        "            if len(suggestions) == n_suggestions:\n",
        "              return suggestions\n",
        "            \n",
        "      return suggestions"
      ],
      "metadata": {
        "id": "KFHu81fFn9Bk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for getting functions of verbs\n",
        "\n",
        "def get_POS(word):\n",
        "  pos_dict = {'ADJECTIVE': 'sıfat', 'VERB': 'fiil', 'ADVERB': 'zarf', 'NOUN': 'isim', 'INTERJECTION': 'nida/ünlem', 'CONJUNCTION': 'bağlaç', 'PREPOSTION': 'edat', 'PRONOUN': 'zamir'}\n",
        "  word_synset = a.getSynSetWithLiteral(word.lower(), 1)\n",
        "  try:\n",
        "    pos = str(word_synset.getPos())\n",
        "    pos = re.sub('Pos.', '', pos)\n",
        "    return pos_dict[pos]\n",
        "  except:\n",
        "    return \"None\"\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "3rgnyxZIPpyz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for combining two example sentence lists\n",
        "\n",
        "def merge_lists(list1, list2):  \n",
        "    merged_list = []\n",
        "    for i in range(len(list1)):\n",
        "        if list1[i] != \"None\":\n",
        "            merged_list.append(list1[i])\n",
        "        elif list2[i] != \"None\":\n",
        "            merged_list.append(list2[i])\n",
        "    return merged_list\n"
      ],
      "metadata": {
        "id": "azrqGgC-A7HV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next 9 cells gets words, definitions, synonyms, example sentences, functions; and must be run in order (1)\n",
        "\n",
        "def select_words_and_meanings(file_list, word_lengths):\n",
        "    words = []\n",
        "    meanings = []\n",
        "    selected_words = []\n",
        "    selected_meanings = []\n",
        "    selected_meanings2 = []\n",
        "    selected_meanings3 = []\n",
        "    \n",
        "    for file in file_list:\n",
        "        with open(file, 'r', encoding='utf-8') as file_object:\n",
        "            lines = file_object.readlines()\n",
        "            for line in lines:\n",
        "                word, meaning = line.strip().split(\": \")                  \n",
        "                if len(word) in word_lengths:\n",
        "                    words.append(word)\n",
        "                    meanings.append(meaning)\n",
        "              \n",
        "    for word_length in word_lengths:\n",
        "        words_with_length = [word for word in words if len(word) == word_length]\n",
        "        selected_words.extend(random.sample(words_with_length, 2))\n",
        "    \n",
        "    for word in selected_words:\n",
        "        indices = [i for i, x in enumerate(words) if x == word]\n",
        "        if len(indices) == 1:\n",
        "            selected_meanings.append(meanings[indices[0]])\n",
        "            selected_meanings2.append(None)\n",
        "            selected_meanings3.append(None)\n",
        "        elif len(indices) >= 2:\n",
        "            selected_meanings.append(meanings[indices[0]])\n",
        "            selected_meanings2.append(meanings[indices[1]])\n",
        "            if len(indices) >= 3:\n",
        "                selected_meanings3.append(meanings[indices[2]])\n",
        "            else:\n",
        "                selected_meanings3.append(None)\n",
        "\n",
        "    return selected_words, selected_meanings, selected_meanings2, selected_meanings3\n",
        "  "
      ],
      "metadata": {
        "id": "tE-JrJck_wsw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Correcting Turkish character confusion \n",
        "\n",
        "class UnicodeTr(str):  #NEW\n",
        "    CHARMAP = {\n",
        "        \"to_upper\": {\n",
        "            \"ı\": \"I\",\n",
        "            \"i\": \"İ\",\n",
        "        },\n",
        "        \"to_lower\": {\n",
        "            \"I\": \"ı\",\n",
        "            \"İ\": \"i\",\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def lower(self):\n",
        "        for key, value in self.CHARMAP.get(\"to_lower\").items():\n",
        "            self = self.replace(key, value)\n",
        "        return self.lower()\n",
        "\n",
        "def apply_unicode_transform(selected_words):\n",
        "    transformed_words = []\n",
        "    for word in selected_words:\n",
        "        transformed_word = UnicodeTr(word).lower()\n",
        "        transformed_words.append(transformed_word)\n",
        "    return transformed_words\n"
      ],
      "metadata": {
        "id": "tmlxAusaAU84"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) | Compound_function\n",
        "\n",
        "def compound_function(transformed_words):\n",
        "    with open('/content/gts (1).json', 'r') as file:\n",
        "        data = json.load(file)\n",
        "        compound_list = []\n",
        "        \n",
        "        for word in selected_words:\n",
        "            found_example = False\n",
        "            for entry in data:\n",
        "                if entry['madde'].lower() == word.lower():\n",
        "                    if entry.get(\"birlesikler\"):\n",
        "                        birlesikler = entry[\"birlesikler\"]\n",
        "                        birlesikler_listesi = birlesikler.split(\", \")\n",
        "                        for birlesik in birlesikler_listesi:\n",
        "                            if word.lower() in birlesik.lower():\n",
        "                                pattern = r'\\b' + re.escape(word) + r'(\\w*)\\b'\n",
        "                                match = re.search(pattern, birlesik, flags=re.IGNORECASE)\n",
        "                                if match:\n",
        "                                    suffix = match.group(1)\n",
        "                                    new_birlesik = re.sub(pattern, \"______\" + suffix, birlesik, flags=re.IGNORECASE)\n",
        "                                    compound_list.append(new_birlesik)\n",
        "                                    found_example = True\n",
        "                                    break\n",
        "                        if found_example:\n",
        "                            break\n",
        "            if not found_example:\n",
        "                compound_list.append(\"None\")\n",
        "                \n",
        "    return compound_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WuDnJ5K5Zmqe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4) Example sentence function (gts.json) | This is combined with gpt examples and assigned to a new list below\n",
        "def example_sentences_function(selected_words):\n",
        "    with open('/content/gts (1).json', 'r') as file:\n",
        "        data = json.load(file)\n",
        "        \n",
        "    example_sentence_list = []\n",
        "    \n",
        "    for word in selected_words:\n",
        "        found_example = False\n",
        "        new_example = \"\"  # Initialize new_example with a default value\n",
        "        \n",
        "        for entry in data:\n",
        "            if entry['madde'].lower() == word.lower():\n",
        "                if entry.get(\"anlamlarListe\"):\n",
        "                    for anlam in entry[\"anlamlarListe\"]:\n",
        "                        if anlam.get(\"orneklerListe\"):\n",
        "                            for ornek in anlam[\"orneklerListe\"]:\n",
        "                                if ornek.get(\"ornek\"):\n",
        "                                    sentence = ornek[\"ornek\"]\n",
        "                                    pattern = r'\\b' + re.escape(word) + r'(\\w*)\\b'\n",
        "                                    match = re.search(pattern, sentence, flags=re.IGNORECASE)\n",
        "                                    if match:\n",
        "                                        suffix = match.group(1)\n",
        "                                        new_example = re.sub(pattern, \"______\" + suffix, sentence, flags=re.IGNORECASE)\n",
        "                                        found_example = True\n",
        "                                        break  # Exit the innermost loop\n",
        "                            if found_example:\n",
        "                                break  # Exit the middle loop\n",
        "                    if found_example:\n",
        "                        break  # Exit the outermost loop\n",
        "        if not found_example:\n",
        "            example_sentence_list.append(\"None\")\n",
        "        else:\n",
        "            example_sentence_list.append(new_example)\n",
        "                        \n",
        "    return example_sentence_list\n",
        "\n"
      ],
      "metadata": {
        "id": "4cXh228nZrIq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.1) Example sentence function requirement\n",
        "\n",
        "def analyze_word(word):\n",
        "    results = morphology.analyze(word)\n",
        "    analyzed_results = []\n",
        "    for result in results:\n",
        "        analyzed_results.append(str(result))\n",
        "    output = re.findall(r'\\[(.*?)\\]', ', '.join(analyzed_results))\n",
        "    output = [element.split(\":\")[0] for element in output]\n",
        "    output = list(OrderedDict.fromkeys(output))\n",
        "    output = [item.strip('\\'') for item in output]\n",
        "    return output[0] if output else \"\"  # Return the first element of output as a string, or an empty string if output is empty"
      ],
      "metadata": {
        "id": "YhkKcxt8MzYU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2) Example sentence function (wordnet) | This is called separately when user requests \n",
        "\n",
        "def get_example_sentence(word):\n",
        "  word_synset = a.getSynSetWithLiteral(word, 1)\n",
        "  s_hint_list = synonym_hint(word)\n",
        "  #example_sentences = ''.join(word_synset.getExample())\n",
        "  new_example = ''\n",
        "  other_example = ''\n",
        "  worD = r\"\\b{}(?:\\w+)?\\b\".format(re.escape(word))#to get the word and possible derivations\n",
        "\n",
        "\n",
        "  if type(word_synset.getExample()) == type(None):\n",
        "    new_example = 'None'\n",
        "    \n",
        "  \n",
        "\n",
        "  else:#for if the sentence uses the original word in the example sentence\n",
        "    example_sentences = ''.join(word_synset.getExample())\n",
        "    example_sentences = example_sentences.lower()\n",
        "    example_tokens = word_tokenize(example_sentences)###NEW####\n",
        "    stem_dict = {}######NEW####\n",
        "    for x in example_tokens:###NEW####\n",
        "      stem_dict[x] = analyze_word(x)###NEW####\n",
        "      if word == stem_dict[x]:###NEW####\n",
        "        new_example = re.sub(x, \"______\", example_sentences)###NEW####\n",
        "\n",
        "\n",
        "\n",
        "    if s_hint_list != []:\n",
        "      for z in s_hint_list:#for example sentences where a synonyms is used in the sentence ###NEW#### change to \"z\"\n",
        "        example_sentences = ''.join(word_synset.getExample())\n",
        "        example_sentences = example_sentences.lower()\n",
        "        hint = r\"\\b{}(?:\\w+)?(?=\\b)\".format(re.escape(z))#the synonym as well as possible derivations\n",
        "        search = ''.join(re.findall(hint, example_sentences))#to see if the synonyms is used or not in the sentence\n",
        "        if search != '':\n",
        "          example_sentences = example_sentences.lower()\n",
        "          example_sentences = example_sentences.lower()\n",
        "          example_tokens = word_tokenize(example_sentences)###NEW####\n",
        "          stem_dict = {}###NEW####\n",
        "          for y in example_tokens:###NEW####\n",
        "            stem_dict[y] = analyze_word(y)###NEW####\n",
        "            if z == stem_dict[y]:###NEW####\n",
        "              new_example = re.sub(y, lambda match: match.group(0).upper(), example_sentences)#to show the synonym in UPPER letters ###NEW####\n",
        "              other_example = re.sub(y, \"______\", example_sentences)#to replace a blank ###NEW####\n",
        "  \n",
        "    new_example = new_example + \"\\nDeğiştirin bu kısmı...\" + \" Tekrar ediyorum...,\\n\" + other_example +\"...\"\n",
        "        \n",
        "  return new_example \n",
        "  "
      ],
      "metadata": {
        "id": "4Y--9Ri7i0I9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (6)\n",
        "file_list = ['4.letters.txt', '5.letters.txt', '6.letters.txt', '7.letters.txt', '8.letters.txt', '9.letters.txt', '10.letters.txt']\n",
        "word_lengths = [4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "selected_words, selected_meanings, selected_meanings2, selected_meanings3 = select_words_and_meanings(file_list, word_lengths)\n"
      ],
      "metadata": {
        "id": "ble4CCaqtXYN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###Important### (6.x1) Instead of this if works run the second one  | (This can take a while) If this does not work either, run (6.1) and jump to (7.1) and active proper lists and run it. Then continue with (7.2)\n",
        "\n",
        "\n",
        "while True:\n",
        "  try:\n",
        "    example_gts = example_sentences_function(word_list)\n",
        "    break\n",
        "  except:\n",
        "    selected_words, selected_meanings, selected_meanings2, selected_meanings3 = select_words_and_meanings(file_list, word_lengths)\n",
        "    word_list = apply_unicode_transform(selected_words)\n"
      ],
      "metadata": {
        "id": "SIovk7WXG3YG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (6.x2) # if this does not work, deactivete this (add \"#\" each line); and activate and run (6.x1). Then continue from the (6.1) (This may give an error on Colab. Unfortunately, We do not know the reason. )\n",
        "#word_list = apply_unicode_transform(selected_words)\n",
        "#example_gts = example_sentences_function(word_list)\n"
      ],
      "metadata": {
        "id": "Zed3L6Yeb-y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (6.1) | These are necessary lists to be defined\n",
        "\n",
        "\n",
        "definition_list = []\n",
        "definition_list2 = []\n",
        "definition_list3 = []\n",
        "synonym_list = []\n",
        "revealed_letters = []\n",
        "\n",
        "total_score = 0\n"
      ],
      "metadata": {
        "id": "_DY7u7_y0jYm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) | Assigning functions to defined lists\n",
        "for i in range(0,14):\n",
        "    definition_list.append(selected_meanings[i])\n",
        "    definition_list2.append(selected_meanings2[i])\n",
        "    definition_list3.append(selected_meanings3[i])"
      ],
      "metadata": {
        "id": "PzSIdMBTZ_vP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.1)\n",
        "\n",
        "for word in word_list: \n",
        "  if synonym_hint(word) != \"None\":\n",
        "    synonym_list.append(synonym_hint(word))\n",
        "  else:\n",
        "    synonym_list.append(\"None\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JtaXAxB3Ct4U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.2)\n",
        "#Just in case lists: If it gives error... (This was to be used during presentation)\n",
        "\n",
        "#word_list = ['post', 'uzay', 'insaf', 'külah', 'kraker', 'dışses', 'babişko', 'gösteri', 'vasistas', 'naftalin', 'uçukkaçık', 'gençirisi', 'çığıraçmak', 'birçırpıda']\n",
        "#definition_list = ['Şömine önü aksesuarı', 'Bütün gök cisimlerinin içinde bulunduğu sonsuz boşluk', 'Acımaya, vicdana veya mantığa dayanan adalet', 'İçine bir tür tatlı konulan yenilebilir kap', 'Gevrek tuzlu bisküvi', 'Sinema ve televizyonda hikâyeye gaipten katılan anlatıcı', 'Erkek ebeveyne yönelik, kimilerine göre samimiyetin sınırlarını zorlayan bir hitap şekli', 'Bir istek veya karşı görüşün, halkın ilgisini çekecek biçimde topluca ve açıkça yapılması', 'Almanlarla ilgili bir şakadan türemiş ve genelleşerek bir pencere türünün adı haline gelmiş söz', 'Kumaş, elbise, halı gibi eşyaları korumakta kullanılan antiseptik bir hidrokarbon', 'Rastgele ve patavatsız bir biçimde hareket eden, deli dolu', 'Battal ergen', 'Bir alanda yeni bir yol, yöntem başlatmak', '“Ele alır almaz, çabucak, tek seferde” anlamında bir söz dizisi']\n",
        "#synonym_list = ['None', ['feza', 'mekan', 'mekan', 'feza'], 'None', 'None', 'None', 'None', 'None', ['demonstrasyon'], 'None', 'None', 'None', 'None', 'None', 'None']\n",
        "#Additional_defs = ['Tilki tilkiliğini bildirinceye kadar ... elden gider', 'Dünya ve atmosferi dışında, evrenin geri kalan bölümü için kullanılan bir söz', 'Vicdana dayalı adalet', 'Konik kâğıt kap', None, None, 'Yüz göz olunmuş erkek ebeveyn unvanı', None, 'Gurbetçi pencere', 'Özellikle güveye karşı kullanılan kokulu bileşik', None, 'Çabuk serpilip büyümüş yaşı küçük kişi', None, None]"
      ],
      "metadata": {
        "id": "AbxJ1DTlYBSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.3)\n",
        "\n",
        "try:\n",
        "  Example_sentence_list = example_sentence(word_list)\n",
        "except:\n",
        "  Example_sentence_list = [\"None\"]*14 "
      ],
      "metadata": {
        "id": "qXNBDfAFOGCA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7.4) | Combining example sentence lists \n",
        "\n",
        "Example_sentences = merge_lists(example_gts, Example_sentence_list)\n",
        "Additional_defs = merge_lists(definition_list2, definition_list3)\n"
      ],
      "metadata": {
        "id": "SGXaNTD0IMEt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List Check\n",
        "\n",
        "#print(word_list)\n",
        "#print(definition_list)\n",
        "#print(synonym_list)\n",
        "#print(Additional_defs)"
      ],
      "metadata": {
        "id": "XHgGUZRfGZN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (8) Assigning origin to Origin_list\n",
        "\n",
        "try:\n",
        "  Origin_list = origin_of_the_words(word_list)\n",
        "  \n",
        "except:\n",
        "  Origin_list = [\"None\"]*14\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "jtsmSGvRTwpO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (8.1) Assigning word functions to Function_list\n",
        "\n",
        "Function_list = function_of_word(word_list)\n"
      ],
      "metadata": {
        "id": "s8X3JRDjIwHY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (8.2) Assigning compound hint to Compound_list\n",
        "\n",
        "Compound_list = compound_function(word_list)"
      ],
      "metadata": {
        "id": "0QbfJOiQY3ir"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (8.3) Assigning structure info (V, N, Adj, etc.) to Structure_list\n",
        "\n",
        "Structure_list = Structure_of_Word(word_list)"
      ],
      "metadata": {
        "id": "nEz4xcaDI3Zx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main function\n",
        "\n",
        "def __main__x():\n",
        "  def countdown_timer():\n",
        "      global total_time, is_paused, resume_time\n",
        "      total_time = 5*60\n",
        "      while total_time > 0:\n",
        "          if is_paused:\n",
        "              time.sleep(1)\n",
        "              continue\n",
        "\n",
        "          mins, secs = divmod(total_time, 60)\n",
        "          timer_display = \"{:02d}:{:02d}\".format(mins, secs)\n",
        "          print(\"\\r\", \"Kalan Zaman:\", timer_display, end=\"\")\n",
        "\n",
        "          time.sleep(1)\n",
        "          total_time -= 1\n",
        "          \n",
        "\n",
        "      print(\"Maalesef efendim, süre sona erdi.\")\n",
        "\n",
        "\n",
        "  def Process():\n",
        "      global total_time, is_paused, resume_time, total_score\n",
        "\n",
        "      global chosen_phrase\n",
        "      chosen_phrase = \"\" \n",
        "      list_active_input = []\n",
        "      chance_list = [2, 3, 4, 7, 9, 5, 25, 65]\n",
        "      print(\"\\n\")\n",
        "      while True:\n",
        "        active_input = input(\"\\n-> \")\n",
        "        \n",
        "        tkn_input = input_tokenizer(active_input.lower())\n",
        "        \n",
        "        if active_input.lower().strip() == word.lower().strip() or word in tkn_input:\n",
        "          phrase_list = [f\"{word} sığıyor mu oraya?\", \"Bana mı soruyorsunuz, cevap mı veriyorsunuz?..\", \"Haydi, şöyle bir cesaret... \", \"Efendim bana sormayın... Ben bir şey diyemem ki...\", \"Ben bilmem...\", \"... emin misiniz..?\", \"Risk alacak mısınız..?\" , \"Buton... Buton..., Unutuyorsunuz basmayı ya da risk almak mı istemiyorsunuz..?\", \"\"]\n",
        "          chosen_phrase = phrase_list[random.randint(0, (len(phrase_list)-1))]\n",
        "          print(\"\\n\",chosen_phrase)\n",
        "\n",
        "        elif any(item in [\"kök\", \"köken\", \"kökenli\", \"kökeni\"] for item in tkn_input):\n",
        "          if Origin_list[round] != \"None\":             \n",
        "            origin_phrases = [f\"Sanırım {Origin_list[round]} olmalı\", f\"Hmm... Bu, sanıyorum {Origin_list[round]}\", f\"{Origin_list[round]}\", f\"{Origin_list[round]} olma ihtimali yüksek\"]\n",
        "            print(\"\\n\",origin_phrases[random.randint(0, (len(origin_phrases) - 1))]) \n",
        "          else:\n",
        "            print(\"\\nMaalesef kökeninden emin değilim...\")\n",
        "\n",
        "        elif active_input.lower().strip() == \"bb\":\n",
        "          if is_paused:\n",
        "              is_paused = False\n",
        "              total_time = resume_time\n",
        "          else:\n",
        "              is_paused = True\n",
        "              resume_time = total_time  \n",
        "          Round_Time(round)\n",
        "          global revealed_letters \n",
        "          revealed_letters = []\n",
        "          break\n",
        "            \n",
        "        elif (active_input.lower().strip() == \"h\") or (\"harf\" in tkn_input and (\"alabilir\" in tkn_input or \"alayım\" in tkn_input)) or (\"harf\" in tkn_input):\n",
        "          current_letters = \" \".join(LetterRequest(word))\n",
        "          print(\"\\n\",current_letters,\"\\n\")\n",
        "          if \"_  \" not in current_letters:\n",
        "            print(\"\\nÜzgünüm, bu sorudan puan alamadınız!\\nOlsun! Üzülmeyin, rakiplerinizin de puan kaybetmeyeceği ne malum...\\nBiz sıradaki soru ile devam edelim...\\n\") \n",
        "            revealed_letters = []\n",
        "            break \n",
        "        \n",
        "        elif any(item in [\"yardım\", \"yardımcı\", \"ipucu\", \"destek\", \"bulamadım\", \"bilemedim\"] for item in tkn_input):\n",
        "            \n",
        "            if len(chance_list) == 0:\n",
        "              negative_phrases = [\"Maalesef, daha fazla yardımcı olamam...\", \"Efendim, daha ne diyeyim ki... Bulursunuz siz bunu...\", \"Başka bir şey gelmiyor ki aklıma...\", \"Maalesef..! Odaklanın, bulursunuz...\\nSoruyu bir daha okuyun isterseniz...\"]\n",
        "              print(\"\\n\",negative_phrases[random.randint(0, (len(negative_phrases)))]) \n",
        "\n",
        "            else:\n",
        "              chance_num = random.choice(chance_list)\n",
        "              \n",
        "              if chance_num % 3 == 0:\n",
        "                print(\"\\nÖrnek cümle verebilirim belki...\")\n",
        "                time.sleep(1)\n",
        "                print(Example_sentences[round])\n",
        "                chance_list.remove(3)\n",
        "                chance_list.remove(9)\n",
        "              \n",
        "              elif chance_num % 5 == 0:\n",
        "                if Compound_list[round] != \"None\":\n",
        "                  print(f\"\\nEfendim buna dikkat... {Compound_list[round]}\")\n",
        "                elif Additional_defs[round] != None:\n",
        "                  print(f\"\\nŞöyle de tanımlanabilir...\\n{Additional_defs[round]}\")\n",
        "                else:\n",
        "                  print(f\"\\nBiraz daha düşünün efendim, tekrar okuyun soruyu...\\n{definition_list[round]}\")\n",
        "\n",
        "                chance_list.remove(5)\n",
        "                chance_list.remove(25)\n",
        "                chance_list.remove(65)\n",
        "\n",
        "\n",
        "              elif chance_num % 2 == 0:\n",
        "                print(\"\\nHmm...\")\n",
        "                time.sleep(1)\n",
        "                try:\n",
        "                  getSynonym(round)\n",
        "                except:\n",
        "                  get_example_sentence(word)\n",
        "                chance_list.remove(2)\n",
        "                chance_list.remove(4)\n",
        "                time.sleep(1)\n",
        "                print(\"\\nDüşünün biraz daha...\")\n",
        "\n",
        "              elif chance_num % 7 == 0:\n",
        "                print(\"\\nHmm... Belki bu yardımcı olabilir...\")\n",
        "                time.sleep(1)\n",
        "                print(\"\\n\",Example_sentences[round])\n",
        "                time.sleep(2)\n",
        "                print(\"Ayrıca...\")\n",
        "                try:\n",
        "                  getSynonym(round)  \n",
        "                except:\n",
        "                  get_example_sentence(word)\n",
        "                chance_list.remove(7)\n",
        "                print(\"\\nOdaklanırsanız bulursunuz bence... Tekrar okuyun...\")\n",
        "\n",
        "        elif (\"eş\" in tkn_input and \"anlamlı\" in tkn_input) or (\"eş\" in tkn_input and \"anlamlısı\" in tkn_input) or (\"eş\" in tkn_input and \"anlam\" in tkn_input) or (\"benzer\" in tkn_input and \"anlamda\" in tkn_input) or (\"eş\" in tkn_input and \"anlamı\" in tkn_input) or (\"benzer\" in tkn_input and \"anlamlı\" in tkn_input):\n",
        "          if getSynonym(round) != \"None\":\n",
        "            print(\"\\n\",getSynonym(round))\n",
        "          else:\n",
        "            print(\"\\nMaalesef aklıma bir şey gelmedi şu an...\")\n",
        "                    \n",
        "            \n",
        "        else:\n",
        "            string_familiarity = [similar_word_hint(word, 3)]\n",
        "            list_synonym = [synonym_hint(word)]\n",
        "            #return string_familiarity\n",
        "            #return list_synonym\n",
        "\n",
        "            if active_input.lower().strip() not in list_active_input:\n",
        "              string_familiarity = similar_word_hint(word, 3)\n",
        "              list_synonym = synonym_hint(word)\n",
        "                \n",
        "            if active_input.lower().strip() in string_familiarity and active_input.lower().strip() in list_synonym:\n",
        "              print(\"\\nHadi bir daha, çok çok yaklaştınız...\")\n",
        "\n",
        "            elif active_input.lower().strip() in string_familiarity or any(item in string_familiarity for item in tkn_input):\n",
        "              print(\"\\nÇok yaklaştınız, hadi birkaç harf değiştirin.\")\n",
        "            \n",
        "            elif active_input.lower() == \"\":\n",
        "              phrases_list_empty = [\"Bir harf alabilirsiniz belki...\", \"...\", \"Zamana dikkat!\", \"Zaman akıyor efendim, bir harf mi istesek..?\", \"...\"]\n",
        "              print(\"\\n\",phrases_list_empty[random.randint(0, (len(phrases_list_else)-1))])\n",
        "\n",
        "            elif active_input.lower().strip() in list_synonym or any(item in list_synonym for item in tkn_input):\n",
        "              synonym_phrases = [\"Bu değil efendim, öteki... Eş anlamlısı lazım bize...\", \"Hadi bir daha deneyin... Bu sefer eş anlamlısını söyleyin...\", f\"{active_input} değil de... Bir benzeri...\\nNeydi o?\\nBize eş anlamlısı lazım...\", \"Yaklaştınız... Aynı anlama gelen başka kelime daha var...\\nNeydi o..?\"]\n",
        "              print(\"\\n\",synonym_phrases[random.randint(0, (len(synonym_phrases) - 1))])\n",
        "\n",
        "            elif active_input.lower().strip() in list_active_input or any(item in list_active_input for item in tkn_input):\n",
        "              print(\"\\nBunu zaten söylediniz, tekrar düşünün...\")        \n",
        "\n",
        "            else: \n",
        "\n",
        "              if editdistance.distance(active_input.lower().strip(), word) <= 2:\n",
        "                if round >= 6:\n",
        "                  distance_phrases = [\"Çok yaklaştınız, acaba bir iki harf mi değiştirsek..?\", f\"{active_input.lower()} doğru muydu yoksa birkaç harf mi farklıydı?\"] \n",
        "                  print(distance_phrases[random.randint(0, len(distance_phrases) - 1)]) \n",
        "\n",
        "              elif \"h\" in list_active_input:\n",
        "                phrases_list_else = [\"Bir harf daha alabilirsiniz belki...\", \"Biraz daha düşünün isterseniz...\", \"Zamana dikkat!\", \"Zaman akıyor efendim, bir harf daha mı istesek..?\"]\n",
        "                print(\"\\n\",phrases_list_else[random.randint(0, (len(phrases_list_else)-1))])\n",
        "              else:\n",
        "                phrases_list_else = [\"Harf mi alsanız..?\", \"Biraz düşünün isterseniz...\", \"Zamana dikkat!\", \"Zaman akıyor efendim, harf mi istesek..?\", \"Hayır, hayır... Dikkatlice tekrar okuyun isterseniz soruyu...\"]\n",
        "                print(\"\\n\",phrases_list_else[random.randint(0, (len(phrases_list_else)-1))])\n",
        "\n",
        "        list_active_input.append(active_input)\n",
        "      chance_list = [2, 3, 4, 7, 9, 5, 25, 65] \n",
        "\n",
        "  # Initialize global variables\n",
        "  total_time = 5 * 60  # 5 minutes in seconds\n",
        "  global is_paused\n",
        "  is_paused = True\n",
        "  resume_time = total_time\n",
        "\n",
        "  # Run countdown timer and take continuous input concurrently\n",
        "  timer_thread = threading.Thread(target=countdown_timer)\n",
        "  timer_thread.start()\n",
        "\n",
        "  for round in range(14):\n",
        "    revealed_letters = []\n",
        "    if round != 0:\n",
        "      ready = input(\"\\nSıradaki soru ile devam etmek için \\\"d\\\" yazınız\\nToplam skorunuzu görmek için \\\"puan\\\" yazınız: \")\n",
        "      if ready.lower().strip() == \"puan\":      \n",
        "        print(f\"\\nŞuanki toplam puanınız: {total_score} \")       \n",
        "        ready = input(\"\\nSıradaki soru ile devam etmek için \\\"d\\\" yazınız: \")\n",
        "\n",
        "        if ready.lower().strip() == \"d\":     \n",
        "          question_number = round+1\n",
        "\n",
        "          global word\n",
        "          word = word_list[round]\n",
        "\n",
        "          Pre_info(question_number, word, total_score, round)\n",
        "\n",
        "          time.sleep(2.3)\n",
        "          letter_count = (((question_number-1) // 2) + 4)\n",
        "          print(\"\")\n",
        "\n",
        "          print(\"_  \"*letter_count, \"\\n\")\n",
        "\n",
        "          print(definition_list[round]) \n",
        "\n",
        "          is_paused = False\n",
        "          Process()\n",
        "          is_paused = True\n",
        "      elif ready.lower().strip() == \"d\":     \n",
        "        question_number = round+1\n",
        "\n",
        "        \n",
        "        word = word_list[round]\n",
        "\n",
        "        Pre_info(question_number, word, total_score, round)\n",
        "\n",
        "        time.sleep(2.3)\n",
        "        letter_count = (((question_number-1) // 2) + 4)\n",
        "        print(\"\")\n",
        "\n",
        "        print(\"_  \"*letter_count, \"\\n\")\n",
        "\n",
        "        print(definition_list[round]) \n",
        "\n",
        "        is_paused = False\n",
        "        Process()\n",
        "        is_paused = True          \n",
        "    else:\n",
        "      question_number = round+1\n",
        "      word = word_list[round]\n",
        "      Pre_info(question_number, word, total_score, round)\n",
        "      time.sleep(3)\n",
        "      is_paused = False\n",
        "      letter_count = (((question_number-1) // 2) + 4)\n",
        "      print(\"\")\n",
        "      print(\"_  \"*letter_count, \"\\n\")\n",
        "      print(definition_list[round]) \n",
        "      is_paused = False\n",
        "      Process()\n",
        "      is_paused = True\n",
        "    revealed_letters = []\n",
        "\n",
        "  # Wait for the timer thread to finish\n",
        "  timer_thread.join()\n"
      ],
      "metadata": {
        "id": "sFJawNHb5j-q"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def redirect_to_survey():\n",
        "    survey_link = \"https://forms.gle/xhRYD5doZLKxumAw7\"  # Replace with your Google Survey link\n",
        "    redirect_html = f'<a href=\"{survey_link}\" target=\"_blank\">Lütfen deneyiminiz değerlendirmek için buraya tıklayın ve anketi doldurun :) </a>'\n",
        "    display(HTML(redirect_html))\n",
        "\n",
        "# Call the function to redirect the user\n"
      ],
      "metadata": {
        "id": "KNPUDul_Qa2a"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run():\n",
        "  Head()\n",
        "  StartCommand()\n",
        "  if start_com.strip() in [\"Ve kayıt\",\"ve kayıt\"]:\n",
        "    Gpt_Prologue()\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "    total_score = 0\n",
        "\n",
        "\n",
        "    __main__x()\n",
        "    time.sleep(3)\n",
        "    end_phrases_super = [f\"Tebrik ederim efendim yarışmayı {total_score} gibi harika bir puanla tamamladınız...\", f\"Bu sorudan sonra kasamız {total_score} puana çıkıyor ve {username} yarışmayı bu müthiş puanla bitiriyor...\\nTebrikler efendim...\" ]\n",
        "    end_phrases_adequate = [f\"Sizi tebrik ederim efendim... Belli ki bugün zor sorular denk gelmiş...\\nYine de yarışmayı {total_score} gibi güzel bir puanla tamamladınız...\", f\"{total_score} puan... Bu zor sorular için gayet iyi bir sonuçla yarışmayı tamamladınız...\\nTebrik ederim...\"]\n",
        "    end_phrases_low = [f\"Bu son soru ile birlikte puanımızı {total_score} puana çıkarttık...\\nBelki hedeflediğimiz değildi ancak üzülmeyin tekrar denersiniz...\\n\\nKapımız size her zaman açık...\", f\"Bu soru ile yarışmayı tamamladık...\\nBelki arzu ettiğimiz puanlara ulaşamadık ancak {total_score} puanın suçunu basiret bağlanmasına atabiliriz\\nve sizi aramızda tekrar görmeyi çok isteriz...\"]\n",
        "\n",
        "    if total_score >= 8500:\n",
        "      print(end_phrases_super[random.randint(0, len(end_phrases_super) - 1)]) \n",
        "    elif total_score >= 7000:\n",
        "      print(end_phrases_adequate[random.randint(0, len(end_phrases_adequate) - 1)]) \n",
        "    else:\n",
        "      print(end_phrases_low[random.randint(0, len(end_phrases_low) - 1)])  \n",
        "\n",
        "    time.sleep(4)\n",
        "    print(\"\\nTekrar görüşmek üzere...\\n\\nSağlıcakla kalın bizi de öksüz bırakmayın...\")\n",
        "    redirect_to_survey()"
      ],
      "metadata": {
        "id": "jziMf5Z45VfV"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After running every cell run this line, uncomment the code (remove \"#\"\").\n",
        "\n",
        "#run()\n"
      ],
      "metadata": {
        "id": "CT2bAnltC36s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}